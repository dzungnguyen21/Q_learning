Initialize weight vector w arbitrarily (e.g., w = 0)
Initialize the behavior policy Ï€ (e.g., ğœ–-greedy policy)
Initialize target policy Ï€' (e.g., deterministic policy that exploits Q)

For each episode:
    Initialize the starting state s
    
    Repeat for each step of the episode until the terminal state:
        1. Choose action a from behavior policy Ï€ (e.g., ğœ–-greedy based on Q(s,a,w))
        2. Take action a, observe reward r and next state s'
        
        3. Select next action a' from the target policy Ï€' (e.g., Ï€'(s') = argmax_a Q(s', a, w))
        
        4. Compute the feature vector Ï•(s, a) for state-action pair (s, a)
        5. Compute the feature vector Ï•(s', a') for state-action pair (s', a')

        6. Estimate the Q-value using the current weight vector w:
            Q(s, a, w) = w^T * Ï•(s, a)
            Q(s', a', w) = w^T * Ï•(s', a')

        7. Compute the TD error (also known as the temporal difference):
            Î´ = r + Î³ * Q(s', a', w) - Q(s, a, w)

        8. Update the weight vector w:
            w = w + Î± * Î´ * Ï•(s, a)

        9. Set s â† s'
    
    Until s is terminal
    
End For

